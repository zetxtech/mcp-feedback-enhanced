#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦å ±å‘Šç”Ÿæˆå™¨
==============

ç”Ÿæˆè©³ç´°çš„ MCP æ¸¬è©¦å ±å‘Šï¼Œæ”¯æŒå¤šç¨®æ ¼å¼è¼¸å‡ºã€‚
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
from dataclasses import dataclass, asdict

from .config import TestConfig, DEFAULT_CONFIG
from .utils import TestUtils
from .validators import TestValidators, ValidationResult
from ..debug import debug_log


@dataclass
class TestReport:
    """æ¸¬è©¦å ±å‘Šæ•¸æ“šçµæ§‹"""
    timestamp: str
    duration: float
    total_scenarios: int
    passed_scenarios: int
    failed_scenarios: int
    success_rate: float
    scenarios: List[Dict[str, Any]]
    validation_summary: Dict[str, Any]
    performance_summary: Dict[str, Any]
    system_info: Dict[str, Any]
    config: Dict[str, Any]
    errors: List[str]
    warnings: List[str]


class TestReporter:
    """æ¸¬è©¦å ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config: Optional[TestConfig] = None):
        self.config = config or DEFAULT_CONFIG
        self.validators = TestValidators(config)
    
    def generate_report(self, test_results: Dict[str, Any]) -> TestReport:
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        start_time = time.time()
        
        # æå–åŸºæœ¬ä¿¡æ¯
        scenarios = test_results.get("results", [])
        total_scenarios = test_results.get("total_scenarios", len(scenarios))
        passed_scenarios = test_results.get("passed_scenarios", 0)
        failed_scenarios = test_results.get("failed_scenarios", 0)
        
        # è¨ˆç®—æˆåŠŸç‡
        success_rate = passed_scenarios / total_scenarios if total_scenarios > 0 else 0
        
        # é©—è­‰æ¸¬è©¦çµæœ
        validation_results = {}
        for i, scenario in enumerate(scenarios):
            validation_results[f"scenario_{i}"] = self.validators.result_validator.validate_test_result(scenario)
        
        validation_summary = self.validators.get_validation_summary(validation_results)
        
        # ç”Ÿæˆæ€§èƒ½æ‘˜è¦
        performance_summary = self._generate_performance_summary(scenarios)
        
        # æ”¶é›†éŒ¯èª¤å’Œè­¦å‘Š
        all_errors = []
        all_warnings = []
        
        for scenario in scenarios:
            all_errors.extend(scenario.get("errors", []))
        
        # è¨ˆç®—ç¸½æŒçºŒæ™‚é–“
        total_duration = 0
        for scenario in scenarios:
            perf = scenario.get("performance", {})
            duration = perf.get("total_duration", 0) or perf.get("total_time", 0)
            total_duration += duration
        
        # å‰µå»ºå ±å‘Š
        report = TestReport(
            timestamp=datetime.now().isoformat(),
            duration=total_duration,
            total_scenarios=total_scenarios,
            passed_scenarios=passed_scenarios,
            failed_scenarios=failed_scenarios,
            success_rate=success_rate,
            scenarios=scenarios,
            validation_summary=validation_summary,
            performance_summary=performance_summary,
            system_info=TestUtils.get_system_info(),
            config=self.config.to_dict(),
            errors=all_errors,
            warnings=all_warnings
        )
        
        debug_log(f"ğŸ“Š æ¸¬è©¦å ±å‘Šç”Ÿæˆå®Œæˆ (è€—æ™‚: {time.time() - start_time:.2f}s)")
        return report
    
    def _generate_performance_summary(self, scenarios: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
        total_duration = 0
        min_duration = float('inf')
        max_duration = 0
        durations = []
        
        memory_usage = []
        
        for scenario in scenarios:
            perf = scenario.get("performance", {})
            
            # è™•ç†æŒçºŒæ™‚é–“
            duration = perf.get("total_duration", 0) or perf.get("total_time", 0)
            if duration > 0:
                total_duration += duration
                min_duration = min(min_duration, duration)
                max_duration = max(max_duration, duration)
                durations.append(duration)
            
            # è™•ç†å…§å­˜ä½¿ç”¨
            memory_diff = perf.get("memory_diff", {})
            if memory_diff:
                memory_usage.append(memory_diff)
        
        # è¨ˆç®—å¹³å‡å€¼
        avg_duration = total_duration / len(durations) if durations else 0
        
        # è¨ˆç®—ä¸­ä½æ•¸
        if durations:
            sorted_durations = sorted(durations)
            n = len(sorted_durations)
            median_duration = (
                sorted_durations[n // 2] if n % 2 == 1
                else (sorted_durations[n // 2 - 1] + sorted_durations[n // 2]) / 2
            )
        else:
            median_duration = 0
        
        return {
            "total_duration": total_duration,
            "total_duration_formatted": TestUtils.format_duration(total_duration),
            "avg_duration": avg_duration,
            "avg_duration_formatted": TestUtils.format_duration(avg_duration),
            "median_duration": median_duration,
            "median_duration_formatted": TestUtils.format_duration(median_duration),
            "min_duration": min_duration if min_duration != float('inf') else 0,
            "min_duration_formatted": TestUtils.format_duration(min_duration if min_duration != float('inf') else 0),
            "max_duration": max_duration,
            "max_duration_formatted": TestUtils.format_duration(max_duration),
            "scenarios_with_performance": len(durations),
            "memory_usage_samples": len(memory_usage)
        }
    
    def save_report(self, report: TestReport, output_path: Optional[Path] = None) -> Path:
        """ä¿å­˜æ¸¬è©¦å ±å‘Š"""
        if output_path is None:
            output_dir = self.config.ensure_report_dir()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"mcp_test_report_{timestamp}.{self.config.report_format}"
            output_path = output_dir / filename
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if self.config.report_format.lower() == "json":
            self._save_json_report(report, output_path)
        elif self.config.report_format.lower() == "html":
            self._save_html_report(report, output_path)
        elif self.config.report_format.lower() == "markdown":
            self._save_markdown_report(report, output_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å ±å‘Šæ ¼å¼: {self.config.report_format}")
        
        debug_log(f"ğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {output_path}")
        return output_path
    
    def _save_json_report(self, report: TestReport, output_path: Path):
        """ä¿å­˜ JSON æ ¼å¼å ±å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
    
    def _save_html_report(self, report: TestReport, output_path: Path):
        """ä¿å­˜ HTML æ ¼å¼å ±å‘Š"""
        html_content = self._generate_html_report(report)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _save_markdown_report(self, report: TestReport, output_path: Path):
        """ä¿å­˜ Markdown æ ¼å¼å ±å‘Š"""
        markdown_content = self._generate_markdown_report(report)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
    
    def _generate_html_report(self, report: TestReport) -> str:
        """ç”Ÿæˆ HTML å ±å‘Š"""
        # ç‹€æ…‹åœ–æ¨™
        status_icon = "âœ…" if report.success_rate == 1.0 else "âŒ" if report.success_rate == 0 else "âš ï¸"
        
        # æ€§èƒ½åœ–è¡¨æ•¸æ“šï¼ˆç°¡åŒ–ç‰ˆï¼‰
        scenario_names = [s.get("scenario_name", f"Scenario {i}") for i, s in enumerate(report.scenarios)]
        scenario_durations = []
        for s in report.scenarios:
            perf = s.get("performance", {})
            duration = perf.get("total_duration", 0) or perf.get("total_time", 0)
            scenario_durations.append(duration)
        
        html = f"""
<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MCP æ¸¬è©¦å ±å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; }}
        .status {{ font-size: 24px; margin: 10px 0; }}
        .success {{ color: #28a745; }}
        .warning {{ color: #ffc107; }}
        .error {{ color: #dc3545; }}
        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
        .card {{ background: #f8f9fa; padding: 15px; border-radius: 6px; border-left: 4px solid #007bff; }}
        .card h3 {{ margin: 0 0 10px 0; color: #333; }}
        .card .value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
        .scenarios {{ margin: 20px 0; }}
        .scenario {{ background: #f8f9fa; margin: 10px 0; padding: 15px; border-radius: 6px; border-left: 4px solid #28a745; }}
        .scenario.failed {{ border-left-color: #dc3545; }}
        .scenario h4 {{ margin: 0 0 10px 0; }}
        .scenario-details {{ display: grid; grid-template-columns: 1fr 1fr; gap: 10px; font-size: 14px; }}
        .errors {{ background: #f8d7da; color: #721c24; padding: 10px; border-radius: 4px; margin: 10px 0; }}
        .performance {{ margin: 20px 0; }}
        .footer {{ text-align: center; margin-top: 30px; color: #666; font-size: 12px; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ§ª MCP æ¸¬è©¦å ±å‘Š</h1>
            <div class="status {'success' if report.success_rate == 1.0 else 'warning' if report.success_rate > 0 else 'error'}">
                {status_icon} æ¸¬è©¦å®Œæˆ
            </div>
            <p>ç”Ÿæˆæ™‚é–“: {report.timestamp}</p>
        </div>
        
        <div class="summary">
            <div class="card">
                <h3>ç¸½æ¸¬è©¦æ•¸</h3>
                <div class="value">{report.total_scenarios}</div>
            </div>
            <div class="card">
                <h3>é€šéæ¸¬è©¦</h3>
                <div class="value" style="color: #28a745;">{report.passed_scenarios}</div>
            </div>
            <div class="card">
                <h3>å¤±æ•—æ¸¬è©¦</h3>
                <div class="value" style="color: #dc3545;">{report.failed_scenarios}</div>
            </div>
            <div class="card">
                <h3>æˆåŠŸç‡</h3>
                <div class="value">{report.success_rate:.1%}</div>
            </div>
            <div class="card">
                <h3>ç¸½è€—æ™‚</h3>
                <div class="value">{report.performance_summary.get('total_duration_formatted', 'N/A')}</div>
            </div>
            <div class="card">
                <h3>å¹³å‡è€—æ™‚</h3>
                <div class="value">{report.performance_summary.get('avg_duration_formatted', 'N/A')}</div>
            </div>
        </div>
        
        <div class="scenarios">
            <h2>ğŸ“‹ æ¸¬è©¦å ´æ™¯è©³æƒ…</h2>
"""
        
        for i, scenario in enumerate(report.scenarios):
            success = scenario.get("success", False)
            scenario_name = scenario.get("scenario_name", f"Scenario {i+1}")
            scenario_desc = scenario.get("scenario_description", "ç„¡æè¿°")
            
            perf = scenario.get("performance", {})
            duration = perf.get("total_duration", 0) or perf.get("total_time", 0)
            duration_str = TestUtils.format_duration(duration) if duration > 0 else "N/A"
            
            steps = scenario.get("steps", {})
            completed_steps = sum(1 for v in steps.values() if v)
            total_steps = len(steps)
            
            errors = scenario.get("errors", [])
            
            html += f"""
            <div class="scenario {'failed' if not success else ''}">
                <h4>{'âœ…' if success else 'âŒ'} {scenario_name}</h4>
                <p>{scenario_desc}</p>
                <div class="scenario-details">
                    <div><strong>ç‹€æ…‹:</strong> {'é€šé' if success else 'å¤±æ•—'}</div>
                    <div><strong>è€—æ™‚:</strong> {duration_str}</div>
                    <div><strong>å®Œæˆæ­¥é©Ÿ:</strong> {completed_steps}/{total_steps}</div>
                    <div><strong>éŒ¯èª¤æ•¸:</strong> {len(errors)}</div>
                </div>
"""
            
            if errors:
                html += '<div class="errors"><strong>éŒ¯èª¤ä¿¡æ¯:</strong><ul>'
                for error in errors:
                    html += f'<li>{error}</li>'
                html += '</ul></div>'
            
            html += '</div>'
        
        html += f"""
        </div>
        
        <div class="performance">
            <h2>ğŸ“Š æ€§èƒ½çµ±è¨ˆ</h2>
            <div class="summary">
                <div class="card">
                    <h3>æœ€å¿«æ¸¬è©¦</h3>
                    <div class="value">{report.performance_summary.get('min_duration_formatted', 'N/A')}</div>
                </div>
                <div class="card">
                    <h3>æœ€æ…¢æ¸¬è©¦</h3>
                    <div class="value">{report.performance_summary.get('max_duration_formatted', 'N/A')}</div>
                </div>
                <div class="card">
                    <h3>ä¸­ä½æ•¸</h3>
                    <div class="value">{report.performance_summary.get('median_duration_formatted', 'N/A')}</div>
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p>MCP Feedback Enhanced æ¸¬è©¦æ¡†æ¶ | ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
    </div>
</body>
</html>
"""
        return html
    
    def _generate_markdown_report(self, report: TestReport) -> str:
        """ç”Ÿæˆ Markdown å ±å‘Š"""
        status_icon = "âœ…" if report.success_rate == 1.0 else "âŒ" if report.success_rate == 0 else "âš ï¸"
        
        md = f"""# ğŸ§ª MCP æ¸¬è©¦å ±å‘Š

{status_icon} **æ¸¬è©¦ç‹€æ…‹**: {'å…¨éƒ¨é€šé' if report.success_rate == 1.0 else 'éƒ¨åˆ†å¤±æ•—' if report.success_rate > 0 else 'å…¨éƒ¨å¤±æ•—'}

**ç”Ÿæˆæ™‚é–“**: {report.timestamp}

## ğŸ“Š æ¸¬è©¦æ‘˜è¦

| æŒ‡æ¨™ | æ•¸å€¼ |
|------|------|
| ç¸½æ¸¬è©¦æ•¸ | {report.total_scenarios} |
| é€šéæ¸¬è©¦ | {report.passed_scenarios} |
| å¤±æ•—æ¸¬è©¦ | {report.failed_scenarios} |
| æˆåŠŸç‡ | {report.success_rate:.1%} |
| ç¸½è€—æ™‚ | {report.performance_summary.get('total_duration_formatted', 'N/A')} |
| å¹³å‡è€—æ™‚ | {report.performance_summary.get('avg_duration_formatted', 'N/A')} |

## ğŸ“‹ æ¸¬è©¦å ´æ™¯è©³æƒ…

"""
        
        for i, scenario in enumerate(report.scenarios):
            success = scenario.get("success", False)
            scenario_name = scenario.get("scenario_name", f"Scenario {i+1}")
            scenario_desc = scenario.get("scenario_description", "ç„¡æè¿°")
            
            perf = scenario.get("performance", {})
            duration = perf.get("total_duration", 0) or perf.get("total_time", 0)
            duration_str = TestUtils.format_duration(duration) if duration > 0 else "N/A"
            
            steps = scenario.get("steps", {})
            completed_steps = sum(1 for v in steps.values() if v)
            total_steps = len(steps)
            
            errors = scenario.get("errors", [])
            
            md += f"""### {'âœ…' if success else 'âŒ'} {scenario_name}

**æè¿°**: {scenario_desc}

- **ç‹€æ…‹**: {'é€šé' if success else 'å¤±æ•—'}
- **è€—æ™‚**: {duration_str}
- **å®Œæˆæ­¥é©Ÿ**: {completed_steps}/{total_steps}
- **éŒ¯èª¤æ•¸**: {len(errors)}

"""
            
            if errors:
                md += "**éŒ¯èª¤ä¿¡æ¯**:\n"
                for error in errors:
                    md += f"- {error}\n"
                md += "\n"
        
        md += f"""## ğŸ“Š æ€§èƒ½çµ±è¨ˆ

| æŒ‡æ¨™ | æ•¸å€¼ |
|------|------|
| æœ€å¿«æ¸¬è©¦ | {report.performance_summary.get('min_duration_formatted', 'N/A')} |
| æœ€æ…¢æ¸¬è©¦ | {report.performance_summary.get('max_duration_formatted', 'N/A')} |
| ä¸­ä½æ•¸ | {report.performance_summary.get('median_duration_formatted', 'N/A')} |

## ğŸ”§ ç³»çµ±ä¿¡æ¯

| é …ç›® | å€¼ |
|------|---|
| CPU æ ¸å¿ƒæ•¸ | {report.system_info.get('cpu_count', 'N/A')} |
| ç¸½å…§å­˜ | {report.system_info.get('memory_total', 'N/A')} |
| å¯ç”¨å…§å­˜ | {report.system_info.get('memory_available', 'N/A')} |

---

*å ±å‘Šç”± MCP Feedback Enhanced æ¸¬è©¦æ¡†æ¶ç”Ÿæˆ | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return md
    
    def print_summary(self, report: TestReport):
        """æ‰“å°æ¸¬è©¦æ‘˜è¦åˆ°æ§åˆ¶å°"""
        status_icon = "âœ…" if report.success_rate == 1.0 else "âŒ" if report.success_rate == 0 else "âš ï¸"
        
        print("\n" + "="*60)
        print(f"ğŸ§ª MCP æ¸¬è©¦å ±å‘Šæ‘˜è¦ {status_icon}")
        print("="*60)
        print(f"ğŸ“Š ç¸½æ¸¬è©¦æ•¸: {report.total_scenarios}")
        print(f"âœ… é€šéæ¸¬è©¦: {report.passed_scenarios}")
        print(f"âŒ å¤±æ•—æ¸¬è©¦: {report.failed_scenarios}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {report.success_rate:.1%}")
        print(f"â±ï¸  ç¸½è€—æ™‚: {report.performance_summary.get('total_duration_formatted', 'N/A')}")
        print(f"âš¡ å¹³å‡è€—æ™‚: {report.performance_summary.get('avg_duration_formatted', 'N/A')}")
        
        if report.errors:
            print(f"\nâŒ ç™¼ç¾ {len(report.errors)} å€‹éŒ¯èª¤:")
            for error in report.errors[:5]:  # åªé¡¯ç¤ºå‰5å€‹éŒ¯èª¤
                print(f"   â€¢ {error}")
            if len(report.errors) > 5:
                print(f"   ... é‚„æœ‰ {len(report.errors) - 5} å€‹éŒ¯èª¤")
        
        print("="*60)
